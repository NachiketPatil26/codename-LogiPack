{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepPack3D Training Notebook for 3D Bin Packing\n",
    "\n",
    "This notebook trains a reinforcement learning model based on the DeepPack3D implementation, specifically customized for the 3D bin packing problem with constraints to prevent floating boxes, overlapping, and boxes extending outside the container.\n",
    "\n",
    "## Setup and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install tensorflow==2.10.0\n",
    "!pip install matplotlib seaborn numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone and Import DeepPack3D\n",
    "\n",
    "First, we'll clone the DeepPack3D repository if it's not already available, and then import its modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if DeepPack3D is already available, otherwise clone it\n",
    "if not os.path.exists('DeepPack3D'):\n",
    "    !git clone https://github.com/zgtcktom/DeepPack3D.git\n",
    "\n",
    "# Add DeepPack3D to the Python path\n",
    "sys.path.append('DeepPack3D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DeepPack3D modules\n",
    "try:\n",
    "    from DeepPack3D.env import MultiBinPackerEnv\n",
    "    from DeepPack3D.agent import Agent, HeuristicAgent\n",
    "    from DeepPack3D.geometry import Cuboid\n",
    "    from DeepPack3D.SpacePartitioner import SpacePartitioner\n",
    "    from DeepPack3D.conveyor import Conveyor, FileConveyor, InputConveyor\n",
    "    print(\"Successfully imported DeepPack3D modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing DeepPack3D modules: {e}\")\n",
    "    print(\"Please check the repository structure and paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Custom Environment\n",
    "\n",
    "We'll extend the MultiBinPackerEnv to create a custom environment that specifically addresses our requirements:\n",
    "1. No floating boxes\n",
    "2. No overlapping boxes\n",
    "3. No boxes outside the container\n",
    "4. Optimized space utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBinPackerEnv(MultiBinPackerEnv):\n",
    "    def __init__(self, n_bins=1, size=(32, 32, 32), k=5, max_bins=None, max_items=None, \n",
    "                 replace='all', verbose=False, prealloc_bins=0, prealloc_items=0, \n",
    "                 shuffle=False, use_rotate=True, use_skip=True, \n",
    "                 strict_support=True, min_support_percentage=0.5):\n",
    "        super().__init__(n_bins, size, k, max_bins, max_items, replace, verbose, \n",
    "                        prealloc_bins, prealloc_items, shuffle, use_rotate, use_skip)\n",
    "        \n",
    "        # Additional parameters for strict stability checking\n",
    "        self.strict_support = strict_support\n",
    "        self.min_support_percentage = min_support_percentage\n",
    "    \n",
    "    def placeable_coords(self, packer, h_map, size):\n",
    "        \"\"\"Enhanced version of placeable_coords that enforces stricter stability requirements\"\"\"\n",
    "        xz = []\n",
    "        splits = {}\n",
    "        for split in packer.free_splits:\n",
    "            if (split.top < self.size[1]) or (not split.fit(size)):\n",
    "                continue\n",
    "            x, y, z = split.coord\n",
    "            xz.append((x, z))\n",
    "            splits[(x, z)] = split\n",
    "        xz = set(xz)\n",
    "        \n",
    "        w, h, d = size\n",
    "        xyz = []\n",
    "        for x, z in xz:\n",
    "            placement = h_map[z:z + d, x:x + w]\n",
    "            y = np.amax(placement)\n",
    "            \n",
    "            # Calculate support percentage (how much of the bottom face is supported)\n",
    "            if y > 0 and self.strict_support:  # Only check if not on the ground and strict mode is enabled\n",
    "                support_count = np.count_nonzero(placement == y)\n",
    "                support_percentage = support_count / (d * w)\n",
    "                \n",
    "                # Only allow placement if support percentage meets minimum requirement\n",
    "                if support_percentage >= self.min_support_percentage:\n",
    "                    xyz.append((x, y, z, splits[(x, z)]))\n",
    "            else:\n",
    "                # Original condition for ground placement or when strict mode is disabled\n",
    "                if np.count_nonzero(placement == y) / (d * w) > 0.5:\n",
    "                    xyz.append((x, y, z, splits[(x, z)]))\n",
    "        \n",
    "        return xyz\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Enhanced step function with improved reward calculation\"\"\"\n",
    "        items, h_maps, actions = self.state()\n",
    "        \n",
    "        # item, bin, rotation_placement\n",
    "        i, j, k = action\n",
    "        _, (x, y, z), (w, h, d), _ = actions[i][j][k]\n",
    "        \n",
    "        packer = self.packers[j]\n",
    "        cuboid = Cuboid(x, y, z, w, h, d)\n",
    "        if not packer.add(cuboid):\n",
    "            raise Exception(f'invalid space {cuboid}')\n",
    "        self.conveyor.grab(i)\n",
    "        \n",
    "        next_state = self.state(step=True)\n",
    "        \n",
    "        # Enhanced reward shaping\n",
    "        items, h_maps, actions = next_state\n",
    "        \n",
    "        item = items[i] if i < len(items) else None\n",
    "        h_map = h_maps[j]\n",
    "        \n",
    "        # Volume utilization\n",
    "        volume = np.sum([split.volume for split in packer.splits])\n",
    "        \n",
    "        # Pyramid score (encourages stable configurations)\n",
    "        pyramid = volume / np.sum(h_map) if np.sum(h_map) > 0 else 0\n",
    "        \n",
    "        # Compactness score (encourages dense packing)\n",
    "        max_height = np.amax(h_map) if h_map.size > 0 else 0\n",
    "        compactness = volume / np.prod((packer.size[0], max_height, packer.size[2])) if max_height > 0 else 0\n",
    "        \n",
    "        # Contact score (encourages contact with walls and other items)\n",
    "        contact_score = 0\n",
    "        if x == 0 or x + w == packer.size[0]:  # Contact with X walls\n",
    "            contact_score += 0.1\n",
    "        if z == 0 or z + d == packer.size[2]:  # Contact with Z walls\n",
    "            contact_score += 0.1\n",
    "        if y == 0:  # Contact with floor\n",
    "            contact_score += 0.2\n",
    "        \n",
    "        # Combined reward\n",
    "        reward = 0.3 * pyramid + 0.3 * compactness + 0.4 * contact_score\n",
    "        \n",
    "        # Check if we're done\n",
    "        done = len(self.indices(actions)) == 0\n",
    "        \n",
    "        # Handle bin replacement logic (same as original)\n",
    "        if done:\n",
    "            if self.max_bins != -1 and self.used_bins + 1 > self.max_bins:\n",
    "                for i, packer in enumerate(packer for packer in self.packers if packer.space_utilization() != 0):\n",
    "                    self.used_packers.append(packer)\n",
    "                    loc = self.packers.index(packer)\n",
    "                    if self.verbose:\n",
    "                        print(f'bin {self.used_bins - self.n_bins + i}, loc: {loc}, space util: {packer.space_utilization() * 100:.2f}, packed items: {len(packer.splits)}')\n",
    "                done = True\n",
    "            else:\n",
    "                if self.replace == 'max':\n",
    "                    loc = np.argmax([packer.space_utilization() for packer in self.packers])\n",
    "                    packer = self.packers[loc]\n",
    "                    if self.verbose:\n",
    "                        print(f'bin {self.used_bins - self.n_bins}, loc: {loc}, space util: {packer.space_utilization() * 100:.2f}, packed items: {len(packer.splits)}')\n",
    "\n",
    "                    self.used_packers.append(self.packers[loc])\n",
    "                    self.packers[loc] = SpacePartitioner(self.size)\n",
    "                    self.packers[loc].reset()\n",
    "                    added = 1\n",
    "                    self.used_bins += 1\n",
    "                elif self.replace == 'all':\n",
    "                    added = 0\n",
    "                    while True:\n",
    "                        loc = np.argmax([packer.space_utilization() for packer in self.packers])\n",
    "                        packer = self.packers[loc]\n",
    "                        \n",
    "                        if packer.space_utilization() == 0:\n",
    "                            break\n",
    "                        if self.max_bins != -1 and self.used_bins + 1 > self.max_bins:\n",
    "                            break\n",
    "                        if self.verbose:\n",
    "                            print(f'bin {self.used_bins - self.n_bins}, loc: {loc}, space util: {packer.space_utilization() * 100:.2f}, packed items: {len(packer.splits)}')\n",
    "\n",
    "                        self.used_packers.append(self.packers[loc])\n",
    "                        self.packers[loc] = SpacePartitioner(self.size)\n",
    "                        self.packers[loc].reset()\n",
    "                        added += 1\n",
    "                        self.used_bins += 1\n",
    "                else:\n",
    "                    raise Exception('not implemented')\n",
    "                \n",
    "                next_state = self.state(step=True)\n",
    "        \n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def indices(self, actions):\n",
    "        \"\"\"Helper method to get indices of available actions\"\"\"\n",
    "        return [\n",
    "            (i, j, k) \n",
    "            for i in range(len(actions)) \n",
    "            for j in range(len(actions[i])) \n",
    "            for k in range(len(actions[i][j]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom neural network model for 3D bin packing\n",
    "def create_packing_model(input_shape=(15,), learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Create a neural network model for 3D bin packing with the following architecture:\n",
    "    - Input: State features (container state, item dimensions, etc.)\n",
    "    - Output: Q-value for each action\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(256, activation='relu', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1, activation='linear')  # Q-value output\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='mse'\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRLAgent:\n",
    "    def __init__(self, env, model=None, learning_rate=0.001, gamma=0.95, \n",
    "                 epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995,\n",
    "                 batch_size=32, memory_size=10000, target_update_freq=10):\n",
    "        self.env = env\n",
    "        self.state_size = 15  # Number of state features\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = []  # Experience replay memory\n",
    "        self.memory_size = memory_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.step_count = 0\n",
    "        \n",
    "        # Create models\n",
    "        if model is None:\n",
    "            self.model = create_packing_model(input_shape=(self.state_size,), \n",
    "                                             learning_rate=learning_rate)\n",
    "        else:\n",
    "            self.model = model\n",
    "            \n",
    "        # Create target model for stable training\n",
    "        self.target_model = create_packing_model(input_shape=(self.state_size,), \n",
    "                                                learning_rate=learning_rate)\n",
    "        self.update_target_model()\n",
    "        \n",
    "        # Metrics tracking\n",
    "        self.rewards_history = []\n",
    "        self.space_utilization_history = []\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def update_target_model(self):\n",
    "        \"\"\"Update the target model with weights from the primary model\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay memory\"\"\"\n",
    "        if len(self.memory) >= self.memory_size:\n",
    "            self.memory.pop(0)  # Remove oldest memory if at capacity\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def state_to_features(self, state, action=None):\n",
    "        \"\"\"\n",
    "        Convert environment state and action to feature vector for neural network\n",
    "        \n",
    "        Args:\n",
    "            state: Tuple of (items, h_maps, actions) from environment\n",
    "            action: Optional tuple of (i, j, k) indices for selected action\n",
    "            \n",
    "        Returns:\n",
    "            Feature vector (numpy array)\n",
    "        \"\"\"\n",
    "        items, h_maps, actions = state\n",
    "        \n",
    "        # Container features\n",
    "        container_volume = np.prod(self.env.size)\n",
    "        container_dims = np.array(self.env.size) / max(self.env.size)  # Normalized dimensions\n",
    "        \n",
    "        # Current state features\n",
    "        h_map = h_maps[0]  # Use first bin's height map\n",
    "        avg_height = np.mean(h_map) / self.env.size[1]  # Normalized average height\n",
    "        max_height = np.max(h_map) / self.env.size[1]  # Normalized maximum height\n",
    "        height_variance = np.var(h_map) / (self.env.size[1] ** 2)  # Normalized height variance\n",
    "        \n",
    "        # If action is provided, get specific action features\n",
    "        if action is not None:\n",
    "            i, j, k = action\n",
    "            _, (x, y, z), (w, h, d), _ = actions[i][j][k]\n",
    "            \n",
    "            # Item features\n",
    "            item = items[i]\n",
    "            item_volume = w * h * d / container_volume  # Normalized volume\n",
    "            \n",
    "            # Position features\n",
    "            pos_x = x / self.env.size[0]  # Normalized x position\n",
    "            pos_y = y / self.env.size[1]  # Normalized y position\n",
    "            pos_z = z / self.env.size[2]  # Normalized z position\n",
    "            \n",
    "            # Contact features (walls and floor)\n",
    "            wall_contact = 0\n",
    "            if x == 0 or x + w == self.env.size[0]:  # Contact with X walls\n",
    "                wall_contact += 1\n",
    "            if z == 0 or z + d == self.env.size[2]:  # Contact with Z walls\n",
    "                wall_contact += 1\n",
    "            floor_contact = 1 if y == 0 else 0  # Contact with floor\n",
    "            \n",
    "            # Support feature\n",
    "            placement = h_map[z:z+d, x:x+w]\n",
    "            support = np.count_nonzero(placement == y) / (w * d) if y > 0 else 1.0\n",
    "        else:\n",
    "            # Default values when no action is provided\n",
    "            item_volume = 0\n",
    "            pos_x, pos_y, pos_z = 0, 0, 0\n",
    "            wall_contact, floor_contact = 0, 0\n",
    "            support = 0\n",
    "        \n",
    "        # Remaining items feature\n",
    "        remaining_items = len([item for item in items if item is not None]) / self.env.k\n",
    "        \n",
    "        # Combine all features\n",
    "        features = np.array([\n",
    "            container_dims[0], container_dims[1], container_dims[2],\n",
    "            avg_height, max_height, height_variance,\n",
    "            item_volume,\n",
    "            pos_x, pos_y, pos_z,\n",
    "            wall_contact / 2, floor_contact,\n",
    "            support,\n",
    "            remaining_items,\n",
    "            self.env.used_bins / max(1, self.env.max_bins if self.env.max_bins != -1 else 10)\n",
    "        ])\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Select an action using epsilon-greedy policy\n",
    "        \n",
    "        Args:\n",
    "            state: Current environment state\n",
    "            \n",
    "        Returns:\n",
    "            Selected action indices (i, j, k)\n",
    "        \"\"\"\n",
    "        items, h_maps, actions = state\n",
    "        action_indices = self.env.indices(actions)\n",
    "        \n",
    "        if not action_indices:  # No valid actions\n",
    "            return None\n",
    "        \n",
    "        # Explore: choose random action\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.choice(action_indices)\n",
    "        \n",
    "        # Exploit: choose best action based on Q-values\n",
    "        q_values = []\n",
    "        for i, j, k in action_indices:\n",
    "            action = (i, j, k)\n",
    "            features = self.state_to_features(state, action)\n",
    "            features = features.reshape(1, -1)  # Reshape for model input\n",
    "            q_value = self.model.predict(features, verbose=0)[0][0]\n",
    "            q_values.append((action, q_value))\n",
    "        \n",
    "        # Select action with highest Q-value\n",
    "        best_action, _ = max(q_values, key=lambda x: x[1])\n",
    "        return best_action\n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"Train the model using experience replay\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0  # Not enough memories to train\n",
    "        \n",
    "        # Sample random batch from memory\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        # Prepare training data\n",
    "        states = []\n",
    "        targets = []\n",
    "        \n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            # Convert state and action to features\n",
    "            state_features = self.state_to_features(state, action)\n",
    "            \n",
    "            # Calculate target Q-value\n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                # Get Q-values for all possible next actions\n",
    "                items, h_maps, actions = next_state\n",
    "                next_action_indices = self.env.indices(actions)\n",
    "                \n",
    "                if not next_action_indices:  # No valid next actions\n",
    "                    target = reward\n",
    "                else:\n",
    "                    next_q_values = []\n",
    "                    for i, j, k in next_action_indices:\n",
    "                        next_action = (i, j, k)\n",
    "                        next_features = self.state_to_features(next_state, next_action)\n",
    "                        next_features = next_features.reshape(1, -1)\n",
    "                        next_q = self.target_model.predict(next_features, verbose=0)[0][0]\n",
    "                        next_q_values.append(next_q)\n",
    "                    \n",
    "                    # Use maximum Q-value for next state\n",
    "                    max_next_q = max(next_q_values) if next_q_values else 0\n",
    "                    target = reward + self.gamma * max_next_q\n",
    "            \n",
    "            # Prepare input and target for training\n",
    "            states.append(state_features)\n",
    "            targets.append(target)\n",
    "        \n",
    "        # Train the model\n",
    "        states = np.array(states)\n",
    "        targets = np.array(targets).reshape(-1, 1)\n",
    "        \n",
    "        history = self.model.fit(states, targets, epochs=1, verbose=0, batch_size=self.batch_size)\n",
    "        loss = history.history['loss'][0]\n",
    "        self.loss_history.append(loss)\n",
    "        \n",
    "        # Update target model periodically\n",
    "        self.step_count += 1\n",
    "        if self.step_count % self.target_update_freq == 0:\n",
    "            self.update_target_model()\n",
    "            \n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def train(self, episodes=1000, max_steps=1000, visualize=False, \n",
    "              save_freq=100, model_path='bin_packing_model.h5'):\n",
    "        \"\"\"\n",
    "        Train the agent on the environment\n",
    "        \n",
    "        Args:\n",
    "            episodes: Number of episodes to train\n",
    "            max_steps: Maximum steps per episode\n",
    "            visualize: Whether to visualize the training\n",
    "            save_freq: Frequency to save the model\n",
    "            model_path: Path to save the model\n",
    "            \n",
    "        Returns:\n",
    "            Training history\n",
    "        \"\"\"\n",
    "        training_history = {\n",
    "            'episode_rewards': [],\n",
    "            'episode_steps': [],\n",
    "            'space_utilization': [],\n",
    "            'loss': []\n",
    "        }\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            # Reset environment\n",
    "            state = self.env.reset()\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            \n",
    "            # Episode loop\n",
    "            for step in range(max_steps):\n",
    "                # Select and perform action\n",
    "                action = self.act(state)\n",
    "                if action is None:  # No valid actions\n",
    "                    break\n",
    "                    \n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                \n",
    "                # Store experience in memory\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                \n",
    "                # Train the model\n",
    "                loss = self.replay()\n",
    "                \n",
    "                # Update state\n",
    "                state = next_state\n",
    "                steps += 1\n",
    "                \n",
    "                # Visualize if requested\n",
    "                if visualize and step % 10 == 0:\n",
    "                    self.visualize_state(state, episode, step)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            # Calculate space utilization\n",
    "            space_util = np.mean([packer.space_utilization() for packer in self.env.used_packers]) * 100 if self.env.used_packers else 0\n",
    "            \n",
    "            # Update history\n",
    "            training_history['episode_rewards'].append(total_reward)\n",
    "            training_history['episode_steps'].append(steps)\n",
    "            training_history['space_utilization'].append(space_util)\n",
    "            training_history['loss'].append(np.mean(self.loss_history[-steps:]) if steps > 0 else 0)\n",
    "            \n",
    "            # Print progress\n",
    "            if episode % 10 == 0:\n",
    "                print(f\"Episode {episode}/{episodes}, Reward: {total_reward:.2f}, Steps: {steps}, \"\n",
    "                      f\"Space Util: {space_util:.2f}%, Epsilon: {self.epsilon:.4f}\")\n",
    "                \n",
    "                # Plot training progress\n",
    "                if episode % 50 == 0:\n",
    "                    self.plot_training_progress(training_history)\n",
    "            \n",
    "            # Save model periodically\n",
    "            if episode % save_freq == 0 and episode > 0:\n",
    "                self.model.save(f\"{model_path.split('.')[0]}_{episode}.h5\")\n",
    "                print(f\"Model saved at episode {episode}\")\n",
    "        \n",
    "        # Save final model\n",
    "        self.model.save(model_path)\n",
    "        print(f\"Final model saved to {model_path}\")\n",
    "        \n",
    "        return training_history\n",
    "    \n",
    "    def visualize_state(self, state, episode, step):\n",
    "        \"\"\"Visualize the current state\"\"\"\n",
    "        try:\n",
    "            items, h_maps, actions = state\n",
    "            \n",
    "            # Plot height map\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            \n",
    "            # Plot height map for each bin\n",
    "            for i, h_map in enumerate(h_maps):\n",
    "                plt.subplot(1, len(h_maps), i+1)\n",
    "                sns.heatmap(h_map, cmap='viridis', vmin=0, vmax=self.env.size[1])\n",
    "                plt.title(f\"Bin {i} Height Map\")\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"outputs/state_ep{episode}_step{step}.png\")\n",
    "            plt.close()\n",
    "            \n",
    "            # Optionally render 3D view if DeepPack3D has visualization\n",
    "            for i, packer in enumerate(self.env.packers):\n",
    "                if hasattr(packer, 'render'):\n",
    "                    fig = packer.render()\n",
    "                    plt.savefig(f\"outputs/3d_ep{episode}_step{step}_bin{i}.png\")\n",
    "                    plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Visualization error: {e}\")\n",
    "    \n",
    "    def plot_training_progress(self, history):\n",
    "        \"\"\"Plot training progress metrics\"\"\"\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Plot rewards\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(history['episode_rewards'])\n",
    "        plt.title('Episode Rewards')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        \n",
    "        # Plot steps\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(history['episode_steps'])\n",
    "        plt.title('Episode Steps')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Steps')\n",
    "        \n",
    "        # Plot space utilization\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(history['space_utilization'])\n",
    "        plt.title('Space Utilization')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Utilization %')\n",
    "        \n",
    "        # Plot loss\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(history['loss'])\n",
    "        plt.title('Training Loss')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Loss')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_progress.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for visualizations\n",
    "if not os.path.exists('outputs'):\n",
    "    os.makedirs('outputs')\n",
    "\n",
    "# Create environment with strict support requirements\n",
    "env = CustomBinPackerEnv(\n",
    "    n_bins=1,\n",
    "    size=(32, 32, 32),\n",
    "    k=5,  # Look ahead 5 items\n",
    "    max_bins=10,\n",
    "    strict_support=True,\n",
    "    min_support_percentage=0.6,  # Require 60% support to prevent floating\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Create agent\n",
    "agent = CustomRLAgent(\n",
    "    env=env,\n",
    "    learning_rate=0.001,\n",
    "    gamma=0.95,\n",
    "    epsilon=1.0,\n",
    "    epsilon_min=0.05,\n",
    "    epsilon_decay=0.995,\n",
    "    batch_size=32,\n",
    "    memory_size=10000,\n",
    "    target_update_freq=10\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "training_history = agent.train(\n",
    "    episodes=500,\n",
    "    max_steps=1000,\n",
    "    visualize=True,\n",
    "    save_freq=50,\n",
    "    model_path='bin_packing_model.h5'\n",
    ")\n",
    "\n",
    "# Plot final training results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot rewards\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(training_history['episode_rewards'])\n",
    "plt.title('Episode Rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "\n",
    "# Plot steps\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(training_history['episode_steps'])\n",
    "plt.title('Episode Steps')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Steps')\n",
    "\n",
    "# Plot space utilization\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(training_history['space_utilization'])\n",
    "plt.title('Space Utilization')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Utilization %')\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(training_history['loss'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_training_results.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the trained model\n",
    "def evaluate_model(model_path, num_episodes=10, visualize=True):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model on the bin packing environment\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the trained model\n",
    "        num_episodes: Number of episodes to evaluate\n",
    "        visualize: Whether to visualize the evaluation\n",
    "        \n",
    "    Returns:\n",
    "        Evaluation metrics\n",
    "    \"\"\"\n",
    "    # Load the trained model\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    # Create environment for evaluation\n",
    "    eval_env = CustomBinPackerEnv(\n",
    "        n_bins=1,\n",
    "        size=(32, 32, 32),\n",
    "        k=5,\n",
    "        max_bins=10,\n",
    "        strict_support=True,\n",
    "        min_support_percentage=0.6,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Create agent with loaded model and no exploration\n",
    "    eval_agent = CustomRLAgent(\n",
    "        env=eval_env,\n",
    "        model=model,\n",
    "        epsilon=0.0,  # No exploration during evaluation\n",
    "        epsilon_min=0.0,\n",
    "        epsilon_decay=1.0\n",
    "    )\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    metrics = {\n",
    "        'space_utilization': [],\n",
    "        'items_packed': [],\n",
    "        'bins_used': [],\n",
    "        'rewards': []\n",
    "    }\n",
    "    \n",
    "    # Run evaluation episodes\n",
    "    for episode in range(num_episodes):\n",
    "        state = eval_env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while True:\n",
    "            # Select action\n",
    "            action = eval_agent.act(state)\n",
    "            if action is None:\n",
    "                break\n",
    "                \n",
    "            # Take action\n",
    "            next_state, reward, done = eval_env.step(action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            \n",
    "            # Visualize if requested\n",
    "            if visualize and steps % 5 == 0:\n",
    "                eval_agent.visualize_state(state, episode, steps)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Calculate metrics\n",
    "        space_util = np.mean([packer.space_utilization() for packer in eval_env.used_packers]) * 100 if eval_env.used_packers else 0\n",
    "        items_packed = sum(len(packer.splits) for packer in eval_env.used_packers)\n",
    "        \n",
    "        # Update metrics\n",
    "        metrics['space_utilization'].append(space_util)\n",
    "        metrics['items_packed'].append(items_packed)\n",
    "        metrics['bins_used'].append(eval_env.used_bins)\n",
    "        metrics['rewards'].append(total_reward)\n",
    "        \n",
    "        print(f\"Evaluation Episode {episode+1}/{num_episodes}:\")\n",
    "        print(f\"  Space Utilization: {space_util:.2f}%\")\n",
    "        print(f\"  Items Packed: {items_packed}\")\n",
    "        print(f\"  Bins Used: {eval_env.used_bins}\")\n",
    "        print(f\"  Total Reward: {total_reward:.2f}\")\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_metrics = {\n",
    "        'avg_space_utilization': np.mean(metrics['space_utilization']),\n",
    "        'avg_items_packed': np.mean(metrics['items_packed']),\n",
    "        'avg_bins_used': np.mean(metrics['bins_used']),\n",
    "        'avg_reward': np.mean(metrics['rewards'])\n",
    "    }\n",
    "    \n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Average Space Utilization: {avg_metrics['avg_space_utilization']:.2f}%\")\n",
    "    print(f\"Average Items Packed: {avg_metrics['avg_items_packed']:.2f}\")\n",
    "    print(f\"Average Bins Used: {avg_metrics['avg_bins_used']:.2f}\")\n",
    "    print(f\"Average Reward: {avg_metrics['avg_reward']:.2f}\")\n",
    "    \n",
    "    return metrics, avg_metrics\n",
    "\n",
    "# Evaluate the trained model\n",
    "eval_metrics, avg_metrics = evaluate_model('bin_packing_model.h5', num_episodes=5, visualize=True)\n",
    "\n",
    "# Plot evaluation results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot space utilization for each evaluation episode\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.bar(range(1, len(eval_metrics['space_utilization'])+1), eval_metrics['space_utilization'])\n",
    "plt.axhline(y=avg_metrics['avg_space_utilization'], color='r', linestyle='--', label=f'Avg: {avg_metrics[\"avg_space_utilization\"]:.2f}%')\n",
    "plt.title('Space Utilization')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Utilization %')\n",
    "plt.legend()\n",
    "\n",
    "# Plot items packed for each evaluation episode\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.bar(range(1, len(eval_metrics['items_packed'])+1), eval_metrics['items_packed'])\n",
    "plt.axhline(y=avg_metrics['avg_items_packed'], color='r', linestyle='--', label=f'Avg: {avg_metrics[\"avg_items_packed\"]:.2f}')\n",
    "plt.title('Items Packed')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "# Plot bins used for each evaluation episode\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.bar(range(1, len(eval_metrics['bins_used'])+1), eval_metrics['bins_used'])\n",
    "plt.axhline(y=avg_metrics['avg_bins_used'], color='r', linestyle='--', label=f'Avg: {avg_metrics[\"avg_bins_used\"]:.2f}')\n",
    "plt.title('Bins Used')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "# Plot rewards for each evaluation episode\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.bar(range(1, len(eval_metrics['rewards'])+1), eval_metrics['rewards'])\n",
    "plt.axhline(y=avg_metrics['avg_reward'], color='r', linestyle='--', label=f'Avg: {avg_metrics[\"avg_reward\"]:.2f}')\n",
    "plt.title('Episode Rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('evaluation_results.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model to TensorFlow.js format for use in your web application\n",
    "!pip install tensorflowjs\n",
    "\n",
    "import tensorflowjs as tfjs\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('bin_packing_model.h5')\n",
    "\n",
    "# Create output directory for the TensorFlow.js model\n",
    "if not os.path.exists('tfjs_model'):\n",
    "    os.makedirs('tfjs_model')\n",
    "\n",
    "# Convert the model to TensorFlow.js format\n",
    "tfjs.converters.save_keras_model(model, 'tfjs_model')\n",
    "print(\"Model converted to TensorFlow.js format and saved to 'tfjs_model' directory\")\n",
    "\n",
    "# Create a simple model info file with metadata\n",
    "model_info = {\n",
    "    \"name\": \"3D Bin Packing Model\",\n",
    "    \"version\": \"1.0\",\n",
    "    \"description\": \"Reinforcement learning model for 3D bin packing\",\n",
    "    \"input_shape\": [15],  # Number of input features\n",
    "    \"output_shape\": [1],  # Q-value output\n",
    "    \"date_created\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "\n",
    "with open('tfjs_model/model_info.json', 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(\"Model info saved to 'tfjs_model/model_info.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display instructions for using the trained model in your project\n",
    "from IPython.display import Markdown\n",
    "\n",
    "instructions = \"\"\"\n",
    "## Using the Trained Model in Your Project\n",
    "\n",
    "To use the trained model in your TypeScript/JavaScript project:\n",
    "\n",
    "1. **Copy the exported model files**:\n",
    "   - Copy the entire `tfjs_model` directory to your project's assets or public directory.\n",
    "\n",
    "2. **Install TensorFlow.js in your project**:\n",
    "   ```bash\n",
    "   npm install @tensorflow/tfjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import * as tf from '@tensorflow/tfjs';\n",
    "\n",
    "// Load the model\n",
    "async function loadModel() {\n",
    "  const model = await tf.loadLayersModel('path/to/tfjs_model/model.json');\n",
    "  return model;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Convert state and action to features\n",
    "function stateActionToFeatures(state, action) {\n",
    "  // Implement the same feature extraction logic as in the Python code\n",
    "  // ...\n",
    "  \n",
    "  // Return features as a tensor\n",
    "  return tf.tensor2d([features], [1, 15]);\n",
    "}\n",
    "\n",
    "// Predict Q-value for a state-action pair\n",
    "async function predictQValue(model, state, action) {\n",
    "  const features = stateActionToFeatures(state, action);\n",
    "  const prediction = model.predict(features);\n",
    "  const qValue = prediction.dataSync()[0];\n",
    "  features.dispose();\n",
    "  prediction.dispose();\n",
    "  return qValue;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Example integration with your existing code\n",
    "async function packWithRL(items, container) {\n",
    "  const model = await loadModel();\n",
    "  \n",
    "  // Initialize state\n",
    "  let state = initializeState(items, container);\n",
    "  \n",
    "  // Pack items one by one\n",
    "  while (hasRemainingItems(state)) {\n",
    "    // Generate valid actions\n",
    "    const validActions = generateValidActions(state);\n",
    "    \n",
    "    // Select best action using the model\n",
    "    let bestAction = null;\n",
    "    let bestQValue = -Infinity;\n",
    "    \n",
    "    for (const action of validActions) {\n",
    "      const qValue = await predictQValue(model, state, action);\n",
    "      if (qValue > bestQValue) {\n",
    "        bestQValue = qValue;\n",
    "        bestAction = action;\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    // Apply the selected action\n",
    "    state = applyAction(state, bestAction);\n",
    "  }\n",
    "  \n",
    "  // Return the packed result\n",
    "  return {\n",
    "    packedItems: state.packedItems,\n",
    "    unpackedItems: state.remainingItems,\n",
    "    // ... other result properties\n",
    "  };\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "1. **Run the notebook in Google Colab**:\n",
    "   - Upload this notebook to Google Colab\n",
    "   - Make sure to select a GPU runtime for faster training\n",
    "\n",
    "2. **Training Process**:\n",
    "   - The notebook will train a reinforcement learning model specifically designed to prevent floating boxes, overlapping, and boxes extending outside the container\n",
    "   - Training will take several hours depending on the number of episodes\n",
    "   - Progress will be visualized and models will be saved periodically\n",
    "\n",
    "3. **After Training**:\n",
    "   - The final model will be exported to TensorFlow.js format\n",
    "   - You can download the `tfjs_model` directory and integrate it with your project\n",
    "   - Follow the instructions in the last cell to properly use the model\n",
    "\n",
    "4. **Customization**:\n",
    "   - You can adjust the training parameters, model architecture, and environment settings to suit your specific requirements\n",
    "   - The `min_support_percentage` parameter is particularly important for preventing floating boxes\n",
    "\n",
    "This approach will give you a specialized model that learns from experience and continuously improves as it encounters more packing scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
